# "AI" Usage Policy

**Foreword on terminology:** What is called "AI" nowadays actually refers to the family of large language models (LLM), which are tools based on fuzzy logic and generalized statistical interpolation. Despite the marketing hype, we will avoid the term "AI" here and stick to "LLM", as we think it is wise to keep in mind the nature of these tools and not attribute to them any capacity that might exceed their inherent design.

The Vienna team fully acknowledge that the history of software development has been marked by the introduction of increasingly sophisticated automated tools. We also recognize that heuristic tools based on LLMs have shown real potential to automate certain software development tasks. While keeping in mind the environmental costs of LLMs and some legal and ethical questions around how they were trained, we are therefore open to welcome work based on LLMs here.

However, please remember that Vienna is maintained by a community of humans.

A community of humans who write code by hand, little by little, like masons building a wall brick by brick. If part of the software is defective, then we can modify it, correct it or replace it. A human who does not like a particular feature will fill out an "issue" to suggest a change. And someone who is already working on the project will look at this "issue", try to understand what the person wants, and see if it makes sense. The author of the "issue" can also propose an alternative piece of code that could meet his need, such as “replace your code with this one, it solves the problem.” This is how we build at the same time better software and a community of contributors.

Thus, every discussion, issue, and pull request is read and reviewed by humans (and sometimes also by machines). This is a demarcation point where people interact with each other and where work is done. LLMs can help in writing code or describing an issue, but they are not suited to replace these interactions.

In a perfect world, LLMs would produce high-quality, accurate work every time. But today, that reality depends on the pilot of the LLM. And today, many pilots are simply not good enough. Using LLMs to generate code or issues without human quality control and effort is burdensome for maintainers and can be disrespectful of the spare time they invest in the project. So, until either people get better at them, or the LLMs improve, or both, we must implement strict rules to protect maintainers.

Issues, discussions and translations can use LLM assistance but must have a full human-in-the-loop. This means that any content generated with LLMs must be reviewed by a human before submission. We value brevity and conciseness; therefore, if an LLM generates overly verbose contributions with superfluous elements that could distract from the main topic, those must be trimmed down.

Pull requests created by LLMs must be clearly disclosed as such and can only be submitted in response to existing, accepted issues. Pull requests that do not refer to an accepted issue will be closed. If you want to share code for a non-accepted issue, open a discussion or attach it to an existing one. If the use of an LLM is not disclosed but a maintainer suspects its use, the PR will be closed.

Pull requests created by LLMs must have been fully verified by a human. LLMs must not create hypothetically correct code that hasn't been read and tested by a human. Crucially, you must not allow LLMs to create code for environments or situations that you are unable to manually test.

We love helping developers learn and improve, and hope to continue doing so without wasting our time arguing with an LLM. **Bad "AI" pilots will be banned and might be called out in public. You've been warned.**

Due to potential copyright issues, only very simple multimedia elements (graphics, sounds etc.) generated by LLM will be accepted. In all cases, the contributor must ensure that they hold an appropriate copyright for any submitted media.